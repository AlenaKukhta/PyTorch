{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3204ff24",
   "metadata": {},
   "source": [
    "# Практическое задание к урокам 9. Трансформер и 10. Распознавание лиц и эмоций\n",
    "\n",
    "Задание на 2 пары (воспринимайте как мини-курсач):\n",
    "\n",
    "Нужно написать приложение, которое будет считывать и выводить кадры с веб-камеры. \n",
    "\n",
    "В процессе считывания определять что перед камерой находится человек, задетектировав его лицо на кадре и находить человека в базе, если он там присутствует.\n",
    "\n",
    "Список возможных датасетов для обучения здесь: https://imerit.net/blog/5-million-faces-top-14-free-image-datasets-for-facial-recognition-all-pbm/ (но стоит проявить фантазию)\n",
    "\n",
    "Если что вопросы в тг @kinetikm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1418256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d974c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy\n",
    "import torch\n",
    "import pickle\n",
    "from scipy import ndimage as ndimage\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "import math\n",
    "\n",
    "import cv2\n",
    "# import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "470ec1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from tensorboardX import SummaryWriter\n",
    "except:\n",
    "    # tensorboardX is not installed, just fail silently\n",
    "    class SummaryWriter():\n",
    "        def __init__(self):\n",
    "            pass\n",
    "        def add_scalar(self, tag, scalar_value, global_step=None, walltime=None):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c71d5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using python 3.9, with modules versions\n",
      "----------------------------------------\n",
      "numpy == 1.20.3\n",
      "torch == 1.10.2\n"
     ]
    }
   ],
   "source": [
    "print('Using python {}.{}, with modules versions'.format(sys.version_info.major, sys.version_info.minor))\n",
    "print('-'*40)\n",
    "print('numpy == {}'.format(numpy.__version__))\n",
    "print('torch == {}'.format(torch.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e93b7501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath='./shrec_data.pckl'):\n",
    "    \"\"\"\n",
    "    Returns hand gesture sequences (X) and their associated labels (Y).\n",
    "    Each sequence has two different labels.\n",
    "    The first label  Y describes the gesture class out of 14 possible gestures (e.g. swiping your hand to the right).\n",
    "    The second label Y describes the gesture class out of 28 possible gestures (e.g. swiping your hand to the right with your index pointed, or not pointed).\n",
    "    \"\"\"\n",
    "    file = open(filepath, 'rb')\n",
    "    data = pickle.load(file, encoding='latin1')  # <<---- change to 'latin1' to 'utf8' if the data does not load\n",
    "    file.close()\n",
    "    return data['x_train'], data['x_test'], data['y_train_14'], data['y_train_28'], data['y_test_14'], data['y_test_28']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bf7dfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_sequences_length(x_train, x_test, final_length=100):\n",
    "    \"\"\"\n",
    "    Resize the time series by interpolating them to the same length\n",
    "    \"\"\"\n",
    "    # please use python3. if you still use python2, important note: redefine the classic division operator / by importing it from the __future__ module\n",
    "    x_train = numpy.array([numpy.array([ndimage.zoom(x_i.T[j], final_length / len(x_i), mode='reflect') for j in range(numpy.size(x_i, 1))]).T for x_i in x_train])\n",
    "    x_test  = numpy.array([numpy.array([ndimage.zoom(x_i.T[j], final_length / len(x_i), mode='reflect') for j in range(numpy.size(x_i, 1)) ]).T for x_i in x_test])\n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f57a99e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_dataset(x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28):\n",
    "    \"\"\"Shuffle the train/test data consistently.\"\"\"\n",
    "    # note: add random_state=0 for reproducibility\n",
    "    x_train, y_train_14, y_train_28 = shuffle(x_train, y_train_14, y_train_28)\n",
    "    x_test,  y_test_14,  y_test_28  = shuffle(x_test,  y_test_14,  y_test_28)\n",
    "    return x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3751589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28):\n",
    "    \"\"\"\n",
    "    Preprocess the data as you want: update as you want!\n",
    "        - possible improvement idea: make a PCA here\n",
    "    \"\"\"\n",
    "    x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28 = shuffle_dataset(x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28)\n",
    "    x_train, x_test = resize_sequences_length(x_train, x_test, final_length=100)\n",
    "    return x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f689883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_pytorch_tensors(x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28):\n",
    "    # as numpy\n",
    "    y_train_14, y_train_28, y_test_14, y_test_28 = numpy.array(y_train_14), numpy.array(y_train_28), numpy.array(y_test_14), numpy.array(y_test_28)\n",
    "    \n",
    "    # -- REQUIRED by the pytorch loss function implementation --\n",
    "    # Remove 1 to all classes items (1-14 => 0-13 and 1-28 => 0-27)\n",
    "    y_train_14, y_train_28, y_test_14, y_test_28 = y_train_14 - 1, y_train_28 - 1, y_test_14 - 1, y_test_28 - 1\n",
    "    \n",
    "    # as torch\n",
    "    x_train, x_test = torch.from_numpy(x_train), torch.from_numpy(x_test)\n",
    "    y_train_14, y_train_28, y_test_14, y_test_28 = torch.from_numpy(y_train_14), torch.from_numpy(y_train_28), torch.from_numpy(y_test_14), torch.from_numpy(y_test_28)\n",
    "\n",
    "    # -- REQUIRED by the pytorch loss function implementation --\n",
    "    # correct the data type (for the loss function used)\n",
    "    x_train, x_test = x_train.type(torch.FloatTensor), x_test.type(torch.FloatTensor)\n",
    "    y_train_14, y_train_28, y_test_14, y_test_28 = y_train_14.type(torch.LongTensor), y_train_28.type(torch.LongTensor), y_test_14.type(torch.LongTensor), y_test_28.type(torch.LongTensor)\n",
    "    \n",
    "    return x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ef84a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(tensor, batch_size=32):\n",
    "    \"\"\"Return a list of (mini) batches\"\"\"\n",
    "    tensor_list = []\n",
    "    length = tensor.shape[0]\n",
    "    i = 0\n",
    "    while True:\n",
    "        if (i + 1) * batch_size >= length:\n",
    "            tensor_list.append(tensor[i * batch_size: length])\n",
    "            return tensor_list\n",
    "        tensor_list.append(tensor[i * batch_size: (i + 1) * batch_size])\n",
    "        i += 1\n",
    "\n",
    "\n",
    "def time_since(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '{:02d}m {:02d}s'.format(int(m), int(s))\n",
    "\n",
    "\n",
    "def get_accuracy(model, x, y_ref):\n",
    "    \"\"\"Get the accuracy of the pytorch model on a batch\"\"\"\n",
    "    acc = 0.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predicted = model(x)\n",
    "        _, predicted = predicted.max(dim=1)\n",
    "        acc = 1.0 * (predicted == y_ref).sum().item() / y_ref.shape[0]\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8069d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandGestureNet(torch.nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, n_channels=66, n_classes=14, dropout_probability=0.2):\n",
    "\n",
    "        super(HandGestureNet, self).__init__()\n",
    "        \n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout_probability = dropout_probability\n",
    "\n",
    "        # Layers ----------------------------------------------\n",
    "        self.all_conv_high = torch.nn.ModuleList([torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(in_channels=1, out_channels=8, kernel_size=7, padding=3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool1d(2),\n",
    "\n",
    "            torch.nn.Conv1d(in_channels=8, out_channels=4, kernel_size=7, padding=3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool1d(2),\n",
    "\n",
    "            torch.nn.Conv1d(in_channels=4, out_channels=4, kernel_size=7, padding=3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=self.dropout_probability),\n",
    "            torch.nn.AvgPool1d(2)\n",
    "        ) for joint in range(n_channels)])\n",
    "\n",
    "        self.all_conv_low = torch.nn.ModuleList([torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(in_channels=1, out_channels=8, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool1d(2),\n",
    "\n",
    "            torch.nn.Conv1d(in_channels=8, out_channels=4, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool1d(2),\n",
    "\n",
    "            torch.nn.Conv1d(in_channels=4, out_channels=4, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=self.dropout_probability),\n",
    "            torch.nn.AvgPool1d(2)\n",
    "        ) for joint in range(n_channels)])\n",
    "\n",
    "        self.all_residual = torch.nn.ModuleList([torch.nn.Sequential(\n",
    "            torch.nn.AvgPool1d(2),\n",
    "            torch.nn.AvgPool1d(2),\n",
    "            torch.nn.AvgPool1d(2)\n",
    "        ) for joint in range(n_channels)])\n",
    "\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=9 * n_channels * 12, out_features=1936),  # <-- 12: depends of the sequences lengths (cf. below)\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=1936, out_features=n_classes)\n",
    "        )\n",
    "\n",
    "        # Initialization --------------------------------------\n",
    "        # Xavier init\n",
    "        for module in itertools.chain(self.all_conv_high, self.all_conv_low, self.all_residual):\n",
    "            for layer in module:\n",
    "                if layer.__class__.__name__ == \"Conv1d\":\n",
    "                    torch.nn.init.xavier_uniform_(layer.weight, gain=torch.nn.init.calculate_gain('relu'))\n",
    "                    torch.nn.init.constant_(layer.bias, 0.1)\n",
    "\n",
    "        for layer in self.fc:\n",
    "            if layer.__class__.__name__ == \"Linear\":\n",
    "                torch.nn.init.xavier_uniform_(layer.weight, gain=torch.nn.init.calculate_gain('relu'))\n",
    "                torch.nn.init.constant_(layer.bias, 0.1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        This function performs the actual computations of the network for a forward pass.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "            input: a tensor of gestures of shape (batch_size, duration, n_channels)\n",
    "                   (where n_channels = 3 * n_joints for 3D pose data)\n",
    "        \"\"\"\n",
    "\n",
    "        # Work on each channel separately\n",
    "        all_features = []\n",
    "\n",
    "        for channel in range(0, self.n_channels):\n",
    "            input_channel = input[:, :, channel]\n",
    "\n",
    "            # Add a dummy (spatial) dimension for the time convolutions\n",
    "            # Conv1D format : (batch_size, n_feature_maps, duration)\n",
    "            input_channel = input_channel.unsqueeze(1)\n",
    "\n",
    "            high = self.all_conv_high[channel](input_channel)\n",
    "            low = self.all_conv_low[channel](input_channel)\n",
    "            ap_residual = self.all_residual[channel](input_channel)\n",
    "\n",
    "            # Time convolutions are concatenated along the feature maps axis\n",
    "            output_channel = torch.cat([\n",
    "                high,\n",
    "                low,\n",
    "                ap_residual\n",
    "            ], dim=1)\n",
    "            all_features.append(output_channel)\n",
    "\n",
    "        # Concatenate along the feature maps axis\n",
    "        all_features = torch.cat(all_features, dim=1)\n",
    "        \n",
    "        # Flatten for the Linear layers\n",
    "        all_features = all_features.view(-1, 9 * self.n_channels * 12)  # <-- 12: depends of the initial sequence length (100).\n",
    "        # If you have shorter/longer sequences, you probably do NOT even need to modify the modify the network architecture:\n",
    "        # resampling your input gesture from T timesteps to 100 timesteps will (surprisingly) probably actually work as well!\n",
    "\n",
    "        # Fully-Connected Layers\n",
    "        output = self.fc(all_features)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7bf1c38",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './shrec_data.pckl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/64/g6jhnpm16ql1vgrrbrbd_yd00000gn/T/ipykernel_9904/2336589360.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_28\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Shuffle sequences and resize sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/64/g6jhnpm16ql1vgrrbrbd_yd00000gn/T/ipykernel_9904/875240298.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0msecond\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mY\u001b[0m \u001b[0mdescribes\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgesture\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mout\u001b[0m \u001b[0mof\u001b[0m \u001b[0;36m28\u001b[0m \u001b[0mpossible\u001b[0m \u001b[0mgestures\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mswiping\u001b[0m \u001b[0myour\u001b[0m \u001b[0mhand\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mright\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0myour\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mpointed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpointed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \"\"\"\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# <<---- change to 'latin1' to 'utf8' if the data does not load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './shrec_data.pckl'"
     ]
    }
   ],
   "source": [
    "# -------------\n",
    "# Data\n",
    "# -------------\n",
    "\n",
    "# Load the dataset\n",
    "x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28 = load_data()\n",
    "\n",
    "# Shuffle sequences and resize sequences\n",
    "x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28 = preprocess_data(x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28)\n",
    "\n",
    "# Convert to pytorch variables\n",
    "x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28 = convert_to_pytorch_tensors(x_train, x_test, y_train_14, y_train_28, y_test_14, y_test_28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d10c479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------\n",
    "# Network instantiation\n",
    "# -------------\n",
    "model = HandGestureNet(n_channels=66, n_classes=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6953d56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "605ebca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------\n",
    "# Training\n",
    "# -------------\n",
    "\n",
    "\n",
    "def train(model, criterion, optimizer,\n",
    "          x_train, y_train, x_test, y_test,\n",
    "          force_cpu=False, num_epochs=5):\n",
    "    \n",
    "    # use a GPU (for speed) if you have one\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() and not force_cpu else torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "    x_train, y_train, x_test, y_test = x_train.to(device), y_train.to(device), x_test.to(device), y_test.to(device)\n",
    "    \n",
    "    # (bonus) log accuracy values to visualize them in tensorboard:\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    # Prepare all mini-batches\n",
    "    x_train_batches = batch(x_train)\n",
    "    y_train_batches = batch(y_train)\n",
    "    \n",
    "    # Training starting time\n",
    "    start = time.time()\n",
    "\n",
    "    print('[INFO] Started to train the model.')\n",
    "    print('Training the model on {}.'.format('GPU' if device == torch.device('cuda') else 'CPU'))\n",
    "    \n",
    "    for ep in range(num_epochs):\n",
    "\n",
    "        # Ensure we're still in training mode\n",
    "        model.train()\n",
    "\n",
    "        current_loss = 0.0\n",
    "\n",
    "        for idx_batch, train_batches in enumerate(zip(x_train_batches, y_train_batches)):\n",
    "\n",
    "            # get a mini-batch of sequences\n",
    "            x_train_batch, y_train_batch = train_batches\n",
    "\n",
    "            # zero the gradient parameters\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            outputs = model(x_train_batch)\n",
    "\n",
    "            # backward + optimize\n",
    "            # backward\n",
    "            loss = criterion(outputs, y_train_batch)\n",
    "            loss.backward()\n",
    "            # optimize\n",
    "            optimizer.step()\n",
    "            # for an easy access\n",
    "            current_loss += loss.item()\n",
    "        \n",
    "        train_acc = get_accuracy(model, x_train, y_train)\n",
    "        test_acc = get_accuracy(model, x_test, y_test)\n",
    "        \n",
    "        writer.add_scalar('data/accuracy_train', train_acc, ep)\n",
    "        writer.add_scalar('data/accuracy_test', test_acc, ep)\n",
    "        print('Epoch #{:03d} | Time elapsed : {} | Loss : {:.4e} | Accuracy_train : {:.4e} | Accuracy_test : {:.4e}'.format(\n",
    "                ep + 1, time_since(start), current_loss, train_acc, test_acc))\n",
    "\n",
    "    print('[INFO] Finished training the model. Total time : {}.'.format(time_since(start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c5e4cc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/64/g6jhnpm16ql1vgrrbrbd_yd00000gn/T/ipykernel_9904/1223209328.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m train(model=model, criterion=criterion, optimizer=optimizer,\n\u001b[0;32m----> 4\u001b[0;31m       \u001b[0mx_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train_14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test_14\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m       num_epochs=num_epochs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "train(model=model, criterion=criterion, optimizer=optimizer,\n",
    "      x_train=x_train, y_train=y_train_14, x_test=x_test, y_test=y_test_14,\n",
    "      num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "287f550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './model_20_epochs.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4ab2ffd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mediapipe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/64/g6jhnpm16ql1vgrrbrbd_yd00000gn/T/ipykernel_9904/678038575.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfacenet_pytorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMTCNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmediapipe\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# mp_drawing = mp.solutions.drawing_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# mp_hands = mp.solutions.hands\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mediapipe'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from facenet_pytorch import MTCNN\n",
    "from PIL import Image\n",
    "import mediapipe as mp\n",
    "# mp_drawing = mp.solutions.drawing_utils\n",
    "# mp_hands = mp.solutions.hands\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f530c0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mp_hands' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/64/g6jhnpm16ql1vgrrbrbd_yd00000gn/T/ipykernel_9904/2072963444.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m with mp_hands.Hands(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmin_detection_confidence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     min_tracking_confidence=0.5) as hands:\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misOpened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mp_hands' is not defined"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "with mp_hands.Hands(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as hands:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    # Flip the image horizontally for a later selfie-view display, and convert\n",
    "    # the BGR image to RGB.\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    results = hands.process(image)\n",
    "\n",
    "    # Draw the hand annotations on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    if results.multi_hand_landmarks:\n",
    "      for hand_landmarks in results.multi_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "    cv2.imshow('MediaPipe Hands', image)\n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "      break\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29b2be7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mediapipe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/64/g6jhnpm16ql1vgrrbrbd_yd00000gn/T/ipykernel_9904/2434663333.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfacenet_pytorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMTCNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmediapipe\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# mp_drawing = mp.solutions.drawing_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# mp_hands = mp.solutions.hands\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mediapipe'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from facenet_pytorch import MTCNN\n",
    "from PIL import Image\n",
    "import mediapipe as mp\n",
    "# mp_drawing = mp.solutions.drawing_utils\n",
    "# mp_hands = mp.solutions.hands\n",
    " \n",
    "\n",
    "# Класс детектирования и обработки лица с веб-камеры \n",
    "class FaceDetector(object):\n",
    "\n",
    "    def __init__(self, mtcnn, mp, resnet,channels=1):\n",
    "        # Создаем объект для считывания потока с веб-камеры(обычно вебкамера идет под номером 0. иногда 1)\n",
    "        self.cap = cv2.VideoCapture(0) \n",
    "        self.mtcnn = mtcnn\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.emodel = resnet\n",
    "        self.channels = channels\n",
    "        self.mp = mp\n",
    "\n",
    "    # Функция рисования найденных параметров на кадре\n",
    "    def _draw(self, frame, boxes, probs, landmarks):\n",
    "        try:\n",
    "            for box, prob, ld in zip(boxes, probs, landmarks):\n",
    "                # Рисуем обрамляющий прямоугольник лица на кадре\n",
    "                cv2.rectangle(frame,\n",
    "                              (int(box[0]), int(box[1])),\n",
    "                              (int(box[2]), int(box[3])),\n",
    "                              (0, 0, 255),\n",
    "                              thickness=2)\n",
    "\n",
    "                # пишем на кадре какая эмоция распознана\n",
    "                \n",
    "\n",
    "                # Рисуем особенные точки\n",
    "#                 cv2.circle(frame, (int(ld[0][0]),int(ld[0][1])), 5, (0, 0, 255), -1)\n",
    "#                 cv2.circle(frame, (int(ld[1][0]),int(ld[1][1])), 5, (0, 0, 255), -1)\n",
    "#                 cv2.circle(frame, (int(ld[2][0]),int(ld[2][1])), 5, (0, 0, 255), -1)\n",
    "#                 cv2.circle(frame, (int(ld[3][0]),int(ld[3][1])), 5, (0, 0, 255), -1)\n",
    "#                 cv2.circle(frame, (int(ld[4][0]),int(ld[4][1])), 5, (0, 0, 255), -1)\n",
    "        except Exception as e:\n",
    "            print('Something wrong im draw function!')\n",
    "            print(f'error : {e}')\n",
    "\n",
    "        return frame\n",
    "    \n",
    "#     # Функция для вырезания лиц с кадра\n",
    "#     @staticmethod\n",
    "#     def crop_faces(frame, boxes):\n",
    "#         faces = []\n",
    "#         for i, box in enumerate(boxes):\n",
    "#             faces.append(frame[int(box[1]-40):int(box[3]+40), \n",
    "#                 int(box[0]-40):int(box[2]+40)])\n",
    "#         return faces\n",
    "    \n",
    "    @staticmethod\n",
    "    def digit_to_classname(digit):\n",
    "        if digit == 0:\n",
    "            return 'sad'\n",
    "        elif digit == 1:\n",
    "            return 'disgust'\n",
    "        elif digit == 2:\n",
    "            return 'happy'\n",
    "        elif digit == 3:\n",
    "            return 'surprise'\n",
    "        elif digit == 4:\n",
    "            return 'neutral'\n",
    "        elif digit == 5:\n",
    "            return 'fear'\n",
    "        elif digit == 6:\n",
    "            return 'angry'\n",
    "       \n",
    "    # Функция в которой будет происходить процесс считывания и обработки каждого кадра\n",
    "    def run(self):\n",
    "        mp_drawing = self.mp.solutions.drawing_utils\n",
    "        mp_hands = self.mp.solutions.hands\n",
    "        with mp_hands.Hands(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as hands:\n",
    "        # Заходим в бесконечный цикл\n",
    "            while True:\n",
    "                # Считываем каждый новый кадр - frame\n",
    "                # ret - логическая переменая. Смысл - считали ли мы кадр с потока или нет\n",
    "                ret, frame = self.cap.read()\n",
    "                try:\n",
    "                    # детектируем расположение лица на кадре, вероятности на сколько это лицо\n",
    "                    # и особенные точки лица\n",
    "                    boxes, probs, landmarks = self.mtcnn.detect(frame, landmarks=True)\n",
    "\n",
    "\n",
    "    #                 # Вырезаем лицо из кадра\n",
    "    #                 face = self.crop_faces(frame, boxes)[0]\n",
    "    #                 # Меняем размер изображения лица для входа в нейронную сеть\n",
    "    #                 face_img = cv2.resize(face,(48,48))\n",
    "    #                 face = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\n",
    "    #                 # Превращаем в 1-канальное серое изображение\n",
    "    #                 face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #                 # Далее мы подготавливаем наш кадр для считывания нс\n",
    "    #                 # Для этого перегоним его в формат pil_image\n",
    "    #                 face = Image.fromarray(face)\n",
    "    #                 #face = face.resize((48,48))\n",
    "    #                 face = np.asarray(face).astype('float')\n",
    "    #                 face = torch.as_tensor(face)\n",
    "\n",
    "\n",
    "    #                 # Превращаем numpy-картинку вырезанного лица в pytorch-тензор\n",
    "    #                 torch_face = face.unsqueeze(0).to(self.device).float()\n",
    "    #                 # Загужаем наш тензор лица в нейронную сеть и получаем предсказание\n",
    "    #                 emotion = self.emodel(torch_face[None, ...])\n",
    "    #                 # Интерпретируем предсказание как строку нашей эмоции\n",
    "    #                 emotion = self.digit_to_classname(emotion[0].argmax().item())\n",
    "\n",
    "                    # Рисуем на кадре\n",
    "                    self._draw(frame, boxes, probs, landmarks)\n",
    "                    vector_list = []\n",
    "\n",
    "                    frame = cv2.cvtColor(cv2.flip(frame, 1), cv2.COLOR_BGR2RGB)\n",
    "                    # To improve performance, optionally mark the image as not writeable to\n",
    "                    # pass by reference.\n",
    "                    frame.flags.writeable = False\n",
    "                    results = hands.process(frame)\n",
    "\n",
    "                    # Draw the hand annotations on the image.\n",
    "                    frame.flags.writeable = True\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                    if results.multi_hand_landmarks:\n",
    "                      for hand_landmarks in results.multi_hand_landmarks:\n",
    "                        for point in range(len(results.multi_hand_landmarks[0].landmark)):\n",
    "                            vector_list.append(hand_landmarks.landmark[point].x)\n",
    "                            vector_list.append(hand_landmarks.landmark[point].y)                        \n",
    "                            vector_list.append(hand_landmarks.landmark[point].z)\n",
    "                        mp_drawing.draw_landmarks(\n",
    "                            frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "#                     cv2.imshow(\"Gray face\", face_img)\n",
    "                    prob = torch.tensor(np.array(vector_list), dtype=torch.float, device=device)\n",
    "                    prob = torch.reshape(prob, (1, x_train.shape[1]))\n",
    "                    result = self.emodel(prob)\n",
    "                    emotion = self.digit_to_classname(result)[0]\n",
    "                    cv2.putText(frame, emotion,\n",
    "                     (int(box[2]), int(box[3])), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                except Exception as e:\n",
    "                    print('Something wrong im main cycle!')\n",
    "                    print(f'error : {e}')\n",
    "\n",
    "                # Показываем кадр в окне, и назвываем его(окно) - 'Face Detection'\n",
    "                cv2.imshow('Hands Detection', frame)\n",
    "\n",
    "\n",
    "                # Функция, которая проверяет нажатие на клавишу 'q'\n",
    "                # Если нажатие произошло - выход из цикла. Конец работы приложения\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "            # Очищаем все объекты opencv, что мы создали\n",
    "        self.cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2bc8dfce",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/64/g6jhnpm16ql1vgrrbrbd_yd00000gn/T/ipykernel_9904/3576974136.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "x_train.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48c1379f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/64/g6jhnpm16ql1vgrrbrbd_yd00000gn/T/ipykernel_9904/1639898771.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmtcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMTCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mourResNet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHandGestureNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m66\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mourResNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_20_epochs.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_getDeviceCount'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             raise AssertionError(\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "mtcnn = MTCNN()\n",
    "device = torch.device(\"cuda\")\n",
    "ourResNet = HandGestureNet(66, 14).to(device)\n",
    "ourResNet.load_state_dict(torch.load('model_20_epochs.pth'))\n",
    "\n",
    "# ourResNet = FERModel(1, 7).to(device)\n",
    "# ourResNet.load_state_dict(torch.load('./models/model2_50_epochs.pth'))\n",
    "\n",
    "\n",
    "ourResNet.eval()\n",
    "# Создаем объект нашего класса приложения\n",
    "fcd = FaceDetector(mtcnn, mp, HandGestureNet)\n",
    "# Запускаем\n",
    "fcd.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
